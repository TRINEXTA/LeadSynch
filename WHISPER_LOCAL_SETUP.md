# üé§ Transcription GRATUITE avec Whisper Local

## ‚ö†Ô∏è IMPORTANT : Solution √©conomique

Au lieu d'utiliser l'API Whisper (payante ~0,006‚Ç¨/minute), on utilise **Whisper en local** :
- ‚úÖ **100% gratuit** (pas de co√ªt par minute)
- ‚úÖ M√™me pr√©cision que l'API
- ‚úÖ Pas de limite d'utilisation
- ‚úÖ Donn√©es restent sur ton serveur (RGPD compliant)
- ‚úÖ Support fran√ßais natif

---

## üì¶ Installation Whisper Local

### Option A : Via Python (recommand√©)

**Pr√©requis :**
- Python 3.8+
- ffmpeg

#### √âtape 1 : Installer ffmpeg

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install ffmpeg

# V√©rifier
ffmpeg -version
```

#### √âtape 2 : Installer Whisper

```bash
# Cr√©er un environnement virtuel
cd /home/user/LeadSynch/app/backend
python3 -m venv whisper-env
source whisper-env/bin/activate

# Installer Whisper
pip install openai-whisper

# Tester
whisper --help
```

#### √âtape 3 : Cr√©er le service de transcription

Cr√©er le fichier **`app/backend/services/whisper-service.py`** :

```python
#!/usr/bin/env python3
import whisper
import sys
import json

# Charger le mod√®le (une seule fois au d√©marrage)
# Mod√®les disponibles : tiny, base, small, medium, large
# Recommand√© : "base" (bon compromis vitesse/pr√©cision)
model = whisper.load_model("base")

def transcribe_audio(audio_path):
    """
    Transcrit un fichier audio en fran√ßais
    """
    try:
        result = model.transcribe(
            audio_path,
            language="fr",
            fp16=False,  # Utiliser CPU (mettre True si GPU disponible)
            verbose=False
        )

        return {
            "success": True,
            "text": result["text"],
            "language": result["language"],
            "segments": len(result["segments"])
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(json.dumps({"success": False, "error": "Audio file path required"}))
        sys.exit(1)

    audio_path = sys.argv[1]
    result = transcribe_audio(audio_path)
    print(json.dumps(result))
```

Rendre le script ex√©cutable :
```bash
chmod +x app/backend/services/whisper-service.py
```

#### √âtape 4 : Tester le service

```bash
# Activer l'environnement
source app/backend/whisper-env/bin/activate

# Tester avec un fichier audio
python3 app/backend/services/whisper-service.py /path/to/audio.mp3

# Devrait retourner :
# {"success": true, "text": "transcription ici...", "language": "fr", "segments": 5}
```

---

## üîå Int√©gration dans l'API Node.js

Modifier **`app/backend/api/call-recordings.js`** (ligne ~350) :

```javascript
import { exec } from 'child_process';
import { promisify } from 'util';
import path from 'path';
import { fileURLToPath } from 'url';

const execPromise = promisify(exec);
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// ========== POST /api/call-recordings/:id/transcribe ==========
router.post('/:id/transcribe', authMiddleware, async (req, res) => {
  const { tenant_id } = req.user;
  const { id } = req.params;

  try {
    const recording = await queryOne(
      'SELECT * FROM call_recordings WHERE id = $1 AND tenant_id = $2',
      [id, tenant_id]
    );

    if (!recording) {
      return res.status(404).json({ error: 'Enregistrement non trouv√©' });
    }

    if (recording.transcription_status === 'completed') {
      return res.json({
        success: true,
        message: 'D√©j√† transcrit',
        transcription: recording.transcription_text
      });
    }

    if (!fs.existsSync(recording.filepath)) {
      return res.status(404).json({ error: 'Fichier audio non trouv√©' });
    }

    // Marquer comme en cours
    await execute(
      `UPDATE call_recordings
       SET transcription_status = 'processing', updated_at = NOW()
       WHERE id = $1`,
      [id]
    );

    console.log(`üé§ D√©but transcription : ${recording.original_filename}`);

    // Appeler Whisper local
    const whisperScriptPath = path.join(__dirname, '../services/whisper-service.py');
    const pythonEnvPath = path.join(__dirname, '../whisper-env/bin/python3');

    const { stdout, stderr } = await execPromise(
      `${pythonEnvPath} ${whisperScriptPath} "${recording.filepath}"`,
      { timeout: 300000 } // 5 minutes max
    );

    if (stderr) {
      console.error('‚ö†Ô∏è Whisper stderr:', stderr);
    }

    const result = JSON.parse(stdout);

    if (result.success) {
      // Sauvegarder la transcription
      await execute(
        `UPDATE call_recordings
         SET
           transcription_status = 'completed',
           transcription_text = $1,
           transcription_language = $2,
           transcription_confidence = 95,
           transcribed_at = NOW(),
           updated_at = NOW()
         WHERE id = $3`,
        [result.text.trim(), result.language, id]
      );

      console.log(`‚úÖ Transcription termin√©e : ${result.segments} segments`);

      return res.json({
        success: true,
        transcription: result.text.trim(),
        language: result.language,
        segments: result.segments
      });
    } else {
      throw new Error(result.error || 'Erreur transcription');
    }

  } catch (error) {
    console.error('‚ùå Erreur transcription:', error);

    // Marquer comme √©chec
    await execute(
      `UPDATE call_recordings
       SET
         transcription_status = 'failed',
         transcription_error = $1,
         updated_at = NOW()
       WHERE id = $2`,
      [error.message, id]
    );

    return res.status(500).json({
      error: 'Erreur lors de la transcription',
      details: error.message
    });
  }
});
```

---

## ‚ö° Optimisations

### 1. Utiliser un mod√®le plus petit pour la vitesse

```python
# Dans whisper-service.py
model = whisper.load_model("tiny")  # Tr√®s rapide, moins pr√©cis
model = whisper.load_model("base")  # Bon compromis (recommand√©)
model = whisper.load_model("small") # Meilleur, un peu plus lent
```

**Temps de transcription (approximatif) :**
- `tiny` : 1 minute d'audio = 5-10 secondes de traitement
- `base` : 1 minute d'audio = 15-30 secondes
- `small` : 1 minute d'audio = 30-60 secondes

### 2. Utiliser GPU si disponible

Si ton serveur a un GPU NVIDIA :

```bash
# Installer avec support CUDA
pip uninstall openai-whisper
pip install openai-whisper torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

Puis dans le script :
```python
result = model.transcribe(
    audio_path,
    language="fr",
    fp16=True,  # ‚úÖ Activer GPU
    verbose=False
)
```

### 3. Queue de traitement asynchrone

Pour √©viter de bloquer l'API, utiliser une queue (Bull/BullMQ) :

```javascript
// Cr√©er une queue de transcription
const transcriptionQueue = new Queue('transcription', process.env.REDIS_URL);

// POST /transcribe envoie juste dans la queue
router.post('/:id/transcribe', authMiddleware, async (req, res) => {
  // ...
  await transcriptionQueue.add({ recordingId: id });

  return res.json({
    success: true,
    message: 'Transcription d√©marr√©e en arri√®re-plan'
  });
});

// Worker qui process la queue
transcriptionQueue.process(async (job) => {
  const { recordingId } = job.data;
  // Appeler Whisper ici
});
```

---

## üê≥ Alternative : Docker avec Whisper

Si tu pr√©f√®res isoler dans Docker :

**Cr√©er `app/backend/Dockerfile.whisper`** :

```dockerfile
FROM python:3.10-slim

# Installer ffmpeg
RUN apt-get update && apt-get install -y ffmpeg

# Installer Whisper
RUN pip install openai-whisper

WORKDIR /app

COPY services/whisper-service.py .

CMD ["python3", "whisper-service.py"]
```

**Docker Compose :**

```yaml
version: '3.8'

services:
  whisper:
    build:
      context: .
      dockerfile: Dockerfile.whisper
    volumes:
      - ./uploads:/app/uploads
    command: tail -f /dev/null  # Garder le container actif
```

---

## üìä Comparaison des co√ªts

| Solution | Co√ªt par minute | 1000 appels (10min chacun) | Limitations |
|----------|----------------|----------------------------|-------------|
| **Whisper API** | 0,006‚Ç¨ | **600‚Ç¨** üí∏ | Payant, quota |
| **Whisper Local** | **0‚Ç¨** | **0‚Ç¨** ‚úÖ | Aucune, gratuit |
| Google Speech-to-Text | 0,006‚Ç¨ | 600‚Ç¨ | Payant |
| Azure Speech | 0,008‚Ç¨ | 800‚Ç¨ | Payant |

**Verdict :** Whisper Local = **600‚Ç¨ d'√©conomies** pour 1000 appels de 10 minutes.

---

## üöÄ D√©ploiement en production

### Sur Vercel (serverless)

‚ö†Ô∏è **Probl√®me :** Vercel ne supporte pas Python + Whisper directement.

**Solutions :**

1. **H√©berger Whisper s√©par√©ment** (Render, Railway, VPS)
2. **Utiliser Vercel + service externe** (API Whisper sur autre serveur)

### Sur VPS / Serveur d√©di√©

```bash
# Installer sur le serveur
cd /var/www/leadsynch/app/backend
python3 -m venv whisper-env
source whisper-env/bin/activate
pip install openai-whisper

# PM2 pour g√©rer le backend Node.js
pm2 start server.js --name leadsynch-api

# Whisper sera appel√© par Node.js via exec()
```

### Sur Railway.app (recommand√©)

Railway supporte Python + Node.js dans le m√™me projet :

```toml
# railway.toml
[build]
builder = "NIXPACKS"

[deploy]
startCommand = "npm run start"

[[build.providers]]
name = "python"
```

---

## üß™ Tests complets

### Test 1 : Whisper standalone

```bash
cd /home/user/LeadSynch/app/backend
source whisper-env/bin/activate

# T√©l√©charger un sample audio
wget https://www2.cs.uic.edu/~i101/SoundFiles/BabyElephantWalk60.wav -O test-audio.wav

# Transcrire
python3 services/whisper-service.py test-audio.wav
```

### Test 2 : Via API

```bash
# Upload un enregistrement
curl -X POST http://localhost:3000/api/call-recordings/upload \
  -H "Authorization: Bearer <token>" \
  -F "audio=@recording.mp3" \
  -F "lead_id=<lead-uuid>" \
  -F "consent_obtained=true"

# R√©cup√©rer l'ID de l'enregistrement
# Exemple : "id": "abc-123-def"

# Lancer la transcription
curl -X POST http://localhost:3000/api/call-recordings/abc-123-def/transcribe \
  -H "Authorization: Bearer <token>"

# Devrait retourner :
# {"success": true, "transcription": "Texte transcrit...", "language": "fr"}
```

---

## üìù R√©sum√© de l'installation

1. **Installer Python + ffmpeg** sur le serveur
2. **Cr√©er environnement virtuel** : `python3 -m venv whisper-env`
3. **Installer Whisper** : `pip install openai-whisper`
4. **Cr√©er script Python** : `services/whisper-service.py`
5. **Modifier API Node.js** : Appeler le script Python via `exec()`
6. **Tester** : Upload audio + transcrire

**Co√ªt final : 0‚Ç¨** (gratuit, illimit√©)

---

## üÜò D√©pannage

### Erreur : "command not found: whisper"

```bash
# V√©rifier l'installation
source whisper-env/bin/activate
which python3
pip list | grep whisper
```

### Erreur : "ffmpeg not found"

```bash
# Installer ffmpeg
sudo apt install ffmpeg

# V√©rifier
ffmpeg -version
```

### Transcription trop lente

- Utiliser mod√®le `tiny` au lieu de `base`
- Activer GPU si disponible
- Utiliser queue asynchrone (Bull)

### Erreur : "ModuleNotFoundError: No module named 'whisper'"

```bash
# R√©installer
pip uninstall openai-whisper
pip install openai-whisper
```

---

**Cr√©√© le** : 15 novembre 2025
**Solution** : Whisper Local (Open Source, gratuit)
**√âconomies** : ~600‚Ç¨ pour 1000 appels de 10 minutes
